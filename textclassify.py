# -*- coding: utf-8 -*-
"""Bản sao của Bản sao của Email_spam_naive_bayes.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1lIuJeO1LeLEzwRzRve3dT9-ObI4Dx94r

# Email Spam Detector

## Import library
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import sys, os, re, csv, codecs, numpy as np, pandas as pd
import tensorflow.keras
import datetime
from tensorflow.keras import backend as K
import tensorflow.keras.optimizers as Optimizer
from tensorflow.keras.utils import to_categorical
from tensorflow.keras.callbacks import ModelCheckpoint, TensorBoard
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation, GRU
from tensorflow.keras.layers import Bidirectional, GlobalMaxPool1D
from tensorflow.keras.models import Model, load_model
# import tensorflow_addons as tfa

from tensorflow.keras import initializers, regularizers, constraints, optimizers, layers
from sklearn.metrics import confusion_matrix as CM
from sklearn.metrics import confusion_matrix, f1_score, precision_score, recall_score
import matplotlib.pyplot as plot
import seaborn as sn
import tensorflow as tf

"""## Import dataset

Business Understanding:

What is mail spam ?

Spam emails are illegal or unsolicited messages sent through to a large number of users. 

Their primary goal is to lure the user into clicking a malicious link or downloading an attachment that is harmful to the user's machine.

Why classifycation mail spam?

to protect user from malicious email

reduce user distractions when receiving unsolicited emails

Goal: 

build a usable spam classification model

The classification is based on the f1 metric, which is more than 0.9
"""

# ## TYPE YOUR CODE for task 2 here:
# from google.colab import drive
# drive.mount('/content/gdrive')

# ## TYPE YOUR CODE for task 3 here:
# %%time
# dataset_path = '/content/gdrive/MyDrive/DSp305/emails_1.csv'

# !cp '{dataset_path}' .

email = pd.read_csv('https://raw.githubusercontent.com/VuongDS/305/main/emails_1.csv')

"""Shape of dataset is (5728, 2).
It's conclude two column:
- 'text' contains the content of the email
- 'spam' is label of dataset
"""

email.shape

email.head(10)

email.isnull().sum()

# !pip -q install wordcloud

from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator

"""## Cleaning the text"""

from gensim.utils import simple_preprocess

email.iloc[:, 0] = email.iloc[:, 0].apply(lambda x: ' '.join(simple_preprocess(x)))

email.tail()

email_spam = email[email['spam'] == 1] 
email_not_spam = email[email['spam'] == 0]

print(email_spam.shape)
print(email_not_spam.shape)

email_spam.head()

"""**Exploratory Data Analysis**"""

# Store word in a list:
text_contain_spam = []
for text in email_spam.text:
  text_contain_spam.append(text)


text_contain_not_spam = []
for text in email_not_spam.text:
  text_contain_not_spam.append(text)

# Conver list to a text file
text_contain_spam = ''.join(text_contain_spam)
text_contain_not_spam = ''.join(text_contain_not_spam)

print(len(text_contain_spam))
print(len(text_contain_not_spam))

import nltk
nltk.download('stopwords')
from nltk.corpus import stopwords

# Create stop words set
stops = set(stopwords.words('english'))

# Update stop words
stops.update(["re", "fw", "fwd", 'ect', 'cc'])

from nltk.tokenize import word_tokenize
from nltk.probability import FreqDist
import collections
nltk.download('punkt')

# Counter word in the text file
c_spam = collections.Counter(word_tokenize(text_contain_spam))
c_not_spam = collections.Counter(word_tokenize(text_contain_not_spam))

most_spam = c_spam.most_common()
most_not_spam = c_not_spam.most_common()

import seaborn as sns

plt.figure(figsize=(7, 7))

x, y= [], []
for word,count in most_spam[:50]:
    if (word not in stops):
        x.append(word)
        y.append(count)
# x = x[:10]        
sns.barplot(x=y[:10],y=x[:10])
plt.title('Email spam')
plt.show()

plt.figure(figsize=(7, 7))
x, y= [], []
for word,count in most_not_spam[:70]:
    if (word not in stops):
        x.append(word)
        y.append(count)
        
sns.barplot(x=y[:10],y=x[:10])
plt.title('Email not spam')
plt.show()

# Create and generate a word cloud image:
wordcloud_spam = WordCloud(stopwords=stops).generate(text_contain_spam)
wordcloud_not_spam = WordCloud(stopwords=stops).generate(text_contain_not_spam)

# Display the generated image:
plt.figure(figsize=(9, 9))
plt.imshow(wordcloud_spam, interpolation='bilinear')
plt.title('Email spam')
plt.axis("off")
plt.show()

plt.figure(figsize=(9, 9))
plt.imshow(wordcloud_not_spam, interpolation='bilinear')
plt.title('Email not spam')
plt.axis("off")
plt.show()

# Statistic of len of each email:
email_spam['text'].str.len().hist()
plt.title('Email spam')
plt.show()

email_not_spam['text'].str.len().hist()
plt.title('Email not spam')
plt.show()

"""## Creating the Bag of Words model"""

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(email['text'], email['spam'], test_size = 0.20, random_state = 0)

X_train = pd.DataFrame(X_train)
X_test = pd.DataFrame(X_test)
y_train = pd.DataFrame(y_train)
y_test = pd.DataFrame(y_test)

from gensim.utils import simple_preprocess

X_train.iloc[:, 0] = X_train.iloc[:, 0].apply(lambda x: ' '.join(simple_preprocess(x)))
X_test.iloc[:, 0] = X_test.iloc[:, 0].apply(lambda x: ' '.join(simple_preprocess(x)))

X_train.head()

y_train.iloc[:, 0] = y_train.iloc[:, 0].apply(lambda x: '__label__' + str(x))
y_test.iloc[:, 0] = y_test.iloc[:, 0].apply(lambda x: '__label__' + str(x))

y_train.head()

# # Text Classification with fastText
# # Importing libraries
# import numpy as np, pandas as pd

# # NLP Preprocessing
# from gensim.utils import simple_preprocess

# # Importing the dataset
# dataset = pd.read_csv('train.csv')[['Body', 'Y']].rename(columns = {'Body': 'questions', 'Y': 'category'})
# ds = pd.read_csv('valid.csv')[['Body', 'Y']].rename(columns = {'Body': 'questions', 'Y': 'category'})

# # NLP Preprocess
# dataset.iloc[:, 0] = dataset.iloc[:, 0].apply(lambda x: ' '.join(simple_preprocess(x)))
# ds.iloc[:, 0] = ds.iloc[:, 0].apply(lambda x: ' '.join(simple_preprocess(x)))

# # Prefixing each row of the category column with '__label__'
# dataset.iloc[:, 1] = dataset.iloc[:, 1].apply(lambda x: '__label__' + x)
# ds.iloc[:, 1] = ds.iloc[:, 1].apply(lambda x: '__label__' + x)

# Create train/test dataset
X_train[['label']] = y_train
X_test[['label']] = y_test

train_dataset = X_train
test_dataset = X_test

train_dataset.head()

# !pip3 -q install fasttext
import fasttext

# Convert dataset to CSV file
train_dataset[['label', 'text']].to_csv('train.txt', 
                                        index = False, 
                                        sep = ' ',
                                        header = None, 
                                        quoting = csv.QUOTE_NONE, 
                                        quotechar = "", 
                                        escapechar = " ")

test_dataset[['label', 'text']].to_csv('test.txt', 
                                        index = False, 
                                        sep = ' ',
                                        header = None, 
                                        quoting = csv.QUOTE_NONE, 
                                        quotechar = "", 
                                        escapechar = " ")

# Training the fastText classifier
model = fasttext.train_supervised('train.txt', wordNgrams = 2)

lst_predict = []
for text in test_dataset.iloc[:, 0]:
  label, pro = model.predict(text)
  lst_predict.append(list(label))
lst_predict = pd.DataFrame(lst_predict)

lst_predict

from sklearn.metrics import f1_score

f1_score(y_test, lst_predict, average='micro')

# Evaluating performance on the entire test file
model.test('test.txt')

# Predicting on a single input
model.predict(test_dataset.iloc[2, 0])

model.predict("I give you a paper on the top of the table")

model.predict("I give you money")

"""**Tuning**"""

from sklearn.model_selection import GridSearchCV

# Training the fastText classifier
model_tune = fasttext.train_supervised(
                                'train.txt', 
                                wordNgrams = 2,
                                dim = 100,
                                lr = 0.1,
                                epoch = 10,
                                autotuneModelSize="200M",
                                autotuneDuration=1200,
                                autotunePredictions=200
                                )

# Evaluating performance on the entire test file
model_tune.test('test.txt')

lst_predict = []
for text in test_dataset.iloc[:, 0]:
  label, pro = model_tune.predict(text)
  lst_predict.append(list(label))
lst_predict = pd.DataFrame(lst_predict)

f1_score(y_test, lst_predict, average='micro')

# Save the trained model
model_tune.save_model('model.bin')

model_tune.predict("I give you a paper on the top of the table")

